# CAM environment configuration

# ---------------------------------------------------------------------------
# Judge configuration (choose one block). The first block (Ollama generate)
# mirrors the built-in defaults; comment it out if you prefer another backend.
# ---------------------------------------------------------------------------

# --- Local llama.cpp (OpenAI-compatible server) ----------------------------
JUDGE_MODE=openai
JUDGE_MODEL=google_medgemma-27b
JUDGE_BASE_URL=http://localhost:8678/v1/chat/completions
JUDGE_API_KEY=
JUDGE_NUM_CTX=8192

# --- Local Ollama (generate endpoint) --------------------------------------
#JUDGE_MODE=ollama
#JUDGE_MODEL=hf.co/bartowski/google_medgemma-27b-it-GGUF:latest
#JUDGE_BASE_URL=http://localhost:11434/api/generate
#JUDGE_BEARER=
# Optional: reduce context window if GPU memory is tight (defaults to 8192)
#JUDGE_NUM_CTX=4096

# --- Local Ollama chat endpoint (uncomment to use) -------------------------
#JUDGE_MODE=ollama_chat
#JUDGE_MODEL=hf.co/bartowski/google_medgemma-27b-it-GGUF:latest
#JUDGE_BASE_URL=http://localhost:11434/api/chat
#JUDGE_BEARER=

# --- OpenAI-compatible server (llama.cpp, OpenAI, vLLM, etc.) --------------
#JUDGE_MODE=openai
#JUDGE_MODEL=gpt-4o-mini
#JUDGE_BASE_URL=http://localhost:8678/v1/chat/completions
#JUDGE_API_KEY=your-judge-api-key

# Optional: override the identifier that appears in reports/audit logs
#JUDGE_ID=custom-judge-name

# ---------------------------------------------------------------------------
# CAM runtime LLM configuration (separate from the judge backend)
# ---------------------------------------------------------------------------

LLM_API_MODE=ollama_chat
OPENAI_ENDPOINT=
OPENAI_API_KEY=

# Ollama / local provider configuration for CAM runtime models
OLLAMA_ENDPOINT=http://localhost:11434/api/generate
OLLAMA_BEARER=

# ---------------------------------------------------------------------------
# Google Gemini API configuration
# ---------------------------------------------------------------------------
GEMINI_API_KEY=your-gemini-key-here
GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta
GEMINI_MODEL=models/gemini-2.0-flash
GEMINI_RPM=10

# ---------------------------------------------------------------------------
# Optional OpenAI-compatible proxy endpoint for CAM runtime
# ---------------------------------------------------------------------------
OPENAI_PROXY_URL=http://localhost:8000/v1

# ---------------------------------------------------------------------------
# CAM model configuration (Ollama model names)
# ---------------------------------------------------------------------------
# Update these to match the tags reported by `ollama list` on your machine.
CAM_MODEL_GEMMA_BASE=gemma3-4B-128k:latest
CAM_MODEL_MEDGEMMA_SMALL=hf.co/bartowski/google_medgemma-4b-it-GGUF:latest
CAM_MODEL_MEDGEMMA_LARGE=google_medgemma-27b
CAM_MODEL_MEDGEMMA_LARGE_API_MODE=openai
CAM_MODEL_MEDGEMMA_LARGE_ENDPOINT=http://localhost:8678/v1/chat/completions
#CAM_MODEL_MEDGEMMA_LARGE_AUTH_ENV_VAR=LLAMA_SERVER_API_KEY
